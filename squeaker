#!/usr/bin/env python3

from pathlib import Path
from urllib.request import Request, urlopen
import argparse
import glob
import hashlib
import json
import math
import os
import shutil
import subprocess
import sys
import tempfile
import time
import zipfile

import logging

def ensuredir(*pieces):
    p = os.path.join(*pieces)
    Path(p).mkdir(parents=True, exist_ok=True)
    return p

def cachedir():
    base = os.environ.get('XDG_CACHE_HOME', os.path.join(Path.home(), '.cache'))
    return os.path.join(base, 'squeaker')

def ensurecachedir(*pieces):
    return ensuredir(cachedir(), *pieces)

def digest(s):
    return hashlib.sha512(s.encode('utf-8')).hexdigest()

def digest_file(path):
    d = hashlib.sha512()
    with open(path, 'rb') as f:
        while True:
            buf = f.read(524288)
            if not buf:
                return d.hexdigest()
            d.update(buf)

def digest_digests(digests):
    d = hashlib.sha512()
    for item in digests:
        d.update(bytearray.fromhex(item))
    return d.hexdigest()

def digest_stage(stage_type, stage_key):
    return digest(f'{stage_type}\n{stage_key}')

def copy_with_progress(expected_str, from_fh, to_fh, extra=''):
    expected = int(expected_str) if expected_str is not None else None
    block_size = math.ceil(expected / 100) if expected is not None else 131072
    total = 0
    def status():
        return f'  {total}/{expected} ({math.floor(100.0 * (total/expected)) if expected is not None else "?"}%){extra}'
    while True:
        logging.info('\r' + status())
        buf = from_fh.read(block_size)
        if not buf:
            break
        total = total + len(buf)
        to_fh.write(buf)
    logging.info('\r' + status() + '\n')

def stage_path(stage_digest):
    return os.path.join(ensurecachedir('stages'), stage_digest)

def load_stage(stage_digest, no_cache=[]):
    infopath = stage_path(stage_digest)
    logging.debug(f'stage cache check {infopath}')
    if os.path.exists(infopath):
        with open(infopath, 'rt') as f:
            info = json.load(f)
        if info['stage_type'] in no_cache:
            logging.info(f'Ignoring (and replacing) cache entry for stage {stage_digest[:20]}')
        else:
            logging.debug(f'  cache hit')
            return info
    else:
        logging.debug(f'  cache miss')
    return None

def image_path(image_digest):
    return os.path.join(ensurecachedir('images'), image_digest)

def stage_lookup(no_cache, stage_type, stage_key_fn, if_absent, extra):
    logging.debug(f'stage_lookup of type {repr(stage_type)}')
    info = load_stage(digest_stage(stage_type, stage_key_fn()), no_cache)
    if info is not None:
        return info

    with tempfile.NamedTemporaryFile(prefix='squeaker-stage-') as output:
        if_absent(output)
        output.flush()
        image_digest = digest_file(output.name)
        shutil.copyfile(output.name, image_path(image_digest))

    final_stage_key = stage_key_fn()
    final_stage_digest = digest_stage(stage_type, final_stage_key)

    info = {
        'image_digest': image_digest,
        'stage_digest': final_stage_digest,
        'stage_type': stage_type,
        'stage_key': final_stage_key,
    }
    for (k, v) in extra.items():
        info[k] = v

    with open(stage_path(final_stage_digest), 'wt') as f:
        json.dump(info, f, indent=2)

    return info

def download(no_cache, url):
    def if_absent(output):
        req = Request(url)
        if req.type == 'file':
            path = req.selector
            with open(path, 'rb') as from_fh:
                logging.info(f'Copying local file {repr(path)} into cache')
                copy_with_progress(expected_str=str(os.path.getsize(path)), from_fh=from_fh, to_fh=output, extra=' ' + path)
        else:
            resp = urlopen(req)
            if resp.status >= 200 and resp.status <= 299:
                logging.info(f'Downloading {url}')
                copy_with_progress(expected_str=resp.headers['content-length'], from_fh=resp, to_fh=output, extra=' ' + url)
            else:
                raise Exception(f'Could not retrieve {url}: HTTP response code {resp.status}:\n{resp.headers}')
    return stage_lookup(no_cache, 'url', lambda: url, if_absent, {
        'url': url
    })

def tag_path(tag):
    return os.path.join(ensurecachedir('tags'), tag)

def load_tag(tag, missing_ok=False):
    path = tag_path(tag)
    if os.path.exists(path):
        with open(path, 'rt') as f:
            return json.load(f)
    if missing_ok:
        return None
    raise Exception(f'Could not load tag {repr(tag)}')

def pretty_stage(stage):
    return stage['image_digest'][:20]

def write_tag(stage, tag, extra):
    logging.info(f'Tagging {pretty_stage(stage)} as {repr(tag)}')
    info = {
        'stage_digest': stage['stage_digest'],
        'image_digest': stage['image_digest'],
        'tag': tag,
    }
    for (k, v) in extra.items():
        info[k] = v
    with open(tag_path(tag), 'wt') as f:
        json.dump(info, f, indent=2)

def extract_with_progress(z, entryname, targetname):
    info = z.getinfo(entryname)
    with z.open(entryname, 'r') as from_fh:
        with open(targetname, 'wb') as to_fh:
            copy_with_progress(str(info.file_size), from_fh=from_fh, to_fh=to_fh, extra=' extracting: ' + entryname)

def archive_with_progress(z, sourcename, entryname):
    with open(sourcename, 'rb') as from_fh:
        with z.open(entryname, 'w') as to_fh:
            copy_with_progress(str(os.path.getsize(sourcename)), from_fh=from_fh, to_fh=to_fh, extra=' archiving: ' + entryname)

def ensure_image_present(info, build_args):
    path = image_path(info['image_digest'])
    if os.path.exists(path):
        logging.debug(f'Image exists for stage {info.get("stage_digest", "???")[:20]} at {path}')
        return info

    if build_args is None:
        raise Exception(f'Cannot find image {repr(path)}')

    logging.info(f'Rebuilding image for stage {info["stage_digest"][:20]}')

    try:
        os.unlink(stage_path(info['stage_digest']))
    except FileNotFoundError:
        pass

    desired_stage_type = info['stage_type']
    if desired_stage_type == 'url':
        return download(build_args.no_cache, info['url'])
    elif desired_stage_type == 'stage':
        parent_info = load_stage(info['parent'])
        if parent_info is None:
            raise Exception(f'Cannot find stage {info["parent"][:20]}')
        return apply_chunk(build_args, parent_info, info['chunk'])
    else:
        raise Exception(f'Unknown stage_type {desired_stage_type}')

def prepare_base(info, build_args):
    info = ensure_image_present(info, build_args)
    path = image_path(info['image_digest'])
    z = zipfile.ZipFile(path)
    names = z.namelist()
    imagename = next((n for n in names if n.endswith('.image')), None)
    if imagename is None:
        raise Exception(f'Base image zip file does not include any *.image files')
    changesname = imagename[:-6] + '.changes'
    if changesname not in names:
        raise Exception(f'Base image zip file contains image {repr(imagename)} but not {repr(changesname)}')
    extract_with_progress(z, imagename, 'squeak.image')
    extract_with_progress(z, changesname, 'squeak.changes')
    return info

def report_time(label, action):
    start_time = time.monotonic()
    result = action()
    end_time = time.monotonic()
    logging.info(f'    {label}: {round(end_time - start_time, 3)} seconds')
    return result

def escape_str(chunk):
    return "'" + chunk.replace("!", "!!").replace("'", "''") + "'"

def incorporate_chunk(args, chunk):
    script_body = f'''
[
    | oldRedirect buildDir logAndQuit |
    buildDir := FileStream detectFile: [FileStream readOnlyFileNamed: 'squeakerDirectory'] do: [:f | f upToEnd].
    FileDirectory setDefaultDirectory: buildDir.
    oldRedirect := TranscriptStream redirectToStdOut.
    TranscriptStream redirectToStdOut: true.
    logAndQuit := [:exn |
        FileStream stderr nextPut: Character cr.
        exn printVerboseOn: FileStream stderr.
        FileStream stderr flush.
        Smalltalk snapshot: false andQuitWithExitCode: 1].
    [
        (
            [
                [
                    Compiler evaluate: {escape_str('['+chunk+']')}
                ] on: SyntaxErrorNotification do: logAndQuit
            ] on: UndeclaredVariableWarning do: [:w |
                w defaultAction.
                Smalltalk snapshot: false andQuitWithExitCode: 1
            ]
        ) value
    ] on: UnhandledError do: [:exn | logAndQuit value: exn exception].
    Transcript flush.
    TranscriptStream redirectToStdOut: oldRedirect.
    Smalltalk garbageCollect; snapshot: true andQuit: true.
] forkAt: Processor lowestPriority + 1 "plus one to avoid having the idle process starve us".
'''
    with open('squeakerDirectory', 'wt') as squeakerDirectory:
        squeakerDirectory.write(args.directory)
    with tempfile.NamedTemporaryFile(prefix='chunk-', suffix='.st') as script:
        script.write(script_body.encode('utf-8'))
        script.flush()
        c = subprocess.run(
            [args.vm, *(['-vm-display-null'] if args.headless else []), 'squeak.image', script.name],
            check=True)

def make_archive(output):
    with zipfile.ZipFile(output, mode='w', compression=zipfile.ZIP_DEFLATED) as z:
        archive_with_progress(z, 'squeak.image', 'squeak.image')
        archive_with_progress(z, 'squeak.changes', 'squeak.changes')

def apply_chunk(args, base_stage, chunk):
    #
    # NOTE: base_stage[0] is *UPDATED* when if_absent is called!
    #       This makes digest_inputs_fn() yield a different answer,
    #       which in turn allows us to repair a partially-cached stage-path.
    #
    base_stage = [base_stage]

    vm_digest = digest(args.vm)
    chunk_digest = digest(chunk)
    digest_inputs_fn = lambda: [base_stage[0]['stage_digest'],
                                base_stage[0]['image_digest'],
                                vm_digest,
                                chunk_digest]
    def if_absent(output):
        with tempfile.TemporaryDirectory(prefix='squeaker-build-') as builddirname:
            os.chdir(builddirname)
            base_stage[0] = prepare_base(base_stage[0], build_args=args)
            logging.info(f' --- {pretty_stage(base_stage[0])}')
            logging.info(('Running:\n' + chunk.replace('\r', '\n')).replace('\n', '\n    '))
            report_time('execution', lambda: incorporate_chunk(args, chunk))
            report_time('archiving', lambda: make_archive(output))
            os.chdir(args.directory)
    return stage_lookup(args.no_cache, 'stage', lambda: digest_digests(digest_inputs_fn()), if_absent, {
        'parent': base_stage[0]['stage_digest'],
        'digest_inputs': digest_inputs_fn(),
        'vm': args.vm,
        'chunk': chunk,
    })

class ChunkReader:
    def __init__(self, fh):
        self.fh = fh
        self.buf = None

    def peek(self):
        if self.buf is None:
            self.buf = self.fh.read(1)
        return self.buf

    def drop(self):
        self.buf = None

    def __iter__(self):
        return self

    def __next__(self):
        chunk = b''
        while self.peek() != b'':
            if self.peek() == b'!':
                self.drop()
                if self.peek() == b'!':
                    self.drop()
                    chunk = chunk + b'!'
                else:
                    return chunk.decode('utf-8')
            else:
                chunk = chunk + self.peek()
                self.drop()
        if chunk == b'':
            raise StopIteration
        else:
            return chunk.decode('utf-8')

def lex_string_literal(s):
    if not (s[0] == "'" and s[-1] == "'"):
        return None
    s = s[1:-1]
    s = s.replace("''", "'")
    return s

def lex_symbol(s):
    if s[0] != '#':
        return None
    return lex_string_literal(s[1:])

def command_build(args):
    args.directory = os.path.abspath(args.directory)
    os.chdir(args.directory)

    base_stage = None

    with open(args.f, 'rb') as squeakerfile:
        for chunk in ChunkReader(squeakerfile):
            chunk = chunk.strip()

            if chunk.startswith('from:'):
                loc = chunk[5:].strip()

                lit = lex_string_literal(loc)
                if lit is not None:
                    base_stage = download(args.no_cache, lit)
                    continue

                srctag = lex_symbol(loc)
                if srctag is not None:
                    base_stage = load_tag(srctag)
                    continue

                raise Exception('Cannot resolve "from:" specifier: ', repr(loc))
            elif chunk == '':
                pass
            else:
                if base_stage is None:
                    raise Exception('No "from:" specifier given')
                base_stage = apply_chunk(args, base_stage, chunk)

    if base_stage is not None:
        base_stage = ensure_image_present(base_stage, build_args=args)
        if args.t:
            write_tag(base_stage, args.t, {})
        print(base_stage['image_digest'])

def command_run(args):
    info = load_tag(args.image, missing_ok=True)
    if info is None:
        matches = glob.glob(image_path(args.image + '*'))
        print(matches)
        if len(matches) == 1:
            info = {'image_digest': os.path.basename(matches[0])}
        elif len(matches) > 1:
            raise Exception(f'Ambiguous image specifier: {repr(args.image)}')
    if info is None:
        raise Exception(f'Could not resolve tag or image {repr(args.image)}')

    old_cwd = os.getcwd()
    with tempfile.TemporaryDirectory(prefix='squeaker-run-') as rundirname:
        try:
            os.chdir(rundirname)
            logging.info(f'Image: {info["image_digest"]}')
            prepare_base(info, build_args=None)
            subprocess.run(
                [args.vm, *(['-vm-display-null'] if args.headless else []), 'squeak.image', *args.args],
                check=True)
        finally:
            os.chdir(old_cwd)

def all_blobs(dirname):
    blobs = []
    for filename in os.listdir(dirname):
        with open(os.path.join(dirname, filename), 'rt') as f:
            blobs.append(json.load(f))
    return blobs

def command_gc(args):
    root_images = set()
    marked_images = set()
    marked_stages = set()

    image_info = {}
    stage_info = {}

    for info in all_blobs(ensurecachedir('stages')):
        image_info.setdefault(info['image_digest'], []).append(info)
        stage_info[info['stage_digest']] = info

    def mark_stage(stage_digest, depth):
        marked_stages.add(stage_digest)
        info = stage_info.get(stage_digest, None)
        if info is None:
            # TODO: fsck-style warnings here
            return
        if depth <= args.keep_intermediate_stages:
            marked_images.add(info['image_digest'])
        if 'parent' in info:
            mark_stage(info['parent'], depth + 1)

    for info in all_blobs(ensurecachedir('tags')):
        marked_images.add(info['image_digest'])
        mark_stage(info['stage_digest'], 0)

    if args.delete_urls is False:
        for info in stage_info.values():
            if info['stage_type'] == 'url':
                mark_stage(info['stage_digest'], 0)
    elif args.delete_urls is True:
        for info in stage_info.values():
            if info['stage_type'] == 'url':
                if info['stage_digest'] in marked_stages:
                    marked_images.add(info['image_digest'])
    elif args.delete_urls == 'all':
        for info in stage_info.values():
            if info['stage_type'] == 'url':
                marked_images.discard(info['image_digest'])
    else:
        raise Exception(f'Invalid delete_urls setting: {repr(args.delete_urls)}')

    all_images = set(os.listdir(ensurecachedir('images')))
    all_stages = set(os.listdir(ensurecachedir('stages')))

    doomed_images = all_images - marked_images
    doomed_stages = all_stages - marked_stages

    logging.info(('Would remove' if args.dry_run else 'Removing') + \
                 f' {len(doomed_images)} image(s) and {len(doomed_stages)} stage(s)')

    for i in doomed_images:
        logging.info(f'    image {i}')
        if not args.dry_run:
            os.unlink(os.path.join(ensurecachedir('images'), i))
    for s in doomed_stages:
        logging.info(f'    stage {s}')
        if not args.dry_run:
            os.unlink(os.path.join(ensurecachedir('stages'), s))

def command_tags(args):
    for info in all_blobs(ensurecachedir('tags')):
        print(info['tag'])

def command_resolve_tag(args):
    print(load_tag(args.tag)['image_digest'])

def command_dot(args):
    print('digraph G {')
    for info in all_blobs(ensurecachedir('tags')):
        print(f'  tag_{info["tag"]} [shape=octagon, style=filled, fillcolor="#ffccff"];')
        print(f'  image_{info["image_digest"][:8]} -> tag_{info["tag"]};')
    stages = dict((s['stage_digest'], s) for s in all_blobs(ensurecachedir('stages')))
    for info in stages.values():
        print(f'  stage_{info["stage_digest"][:8]} [shape=note, style=filled, fillcolor="#ffcccc"];')
        print(f'  image_{info["image_digest"][:8]} [shape=ellipse, style=filled, fillcolor="#ccffcc"];')
        print(f'  stage_{info["stage_digest"][:8]} -> image_{info["image_digest"][:8]};')
        ty = info['stage_type']
        if ty == 'url':
            print(f'  "{info["url"]}" [shape=box, style=filled, fillcolor="#ffffcc"];')
            print(f'  "{info["url"]}" -> stage_{info["stage_digest"][:8]};')
        elif ty == 'stage':
            print(f'  image_{stages[info["parent"]]["image_digest"][:8]} -> stage_{info["stage_digest"][:8]};')
        else:
            pass
    print('}')

class CustomHandler(logging.StreamHandler):
    def emit(self, record):
        if record.msg[0] == '\r':
            record.msg = record.msg[1:]
            self.stream.write('\r')
            if record.msg[-1] == '\n':
                self.terminator = '\n'
                record.msg = record.msg[:-1]
            else:
                self.terminator = ''
        else:
            if self.terminator == '':
                self.stream.write('\n')
            self.terminator = '\n'
        return super().emit(record)

def main(argv):
    app_name = os.path.basename(argv[0])
    argv = argv[1:]

    parser = argparse.ArgumentParser(prog=app_name)
    parser.add_argument('-d', '--debug', action='store_true', default=False,
                        help='Enable debug logging')
    parser.set_defaults(handler=lambda args: parser.print_help())
    sp = parser.add_subparsers()

    p = sp.add_parser('build', help='Build image')
    p.add_argument('-f', metavar='Squeakerfile.st', type=str, default='Squeakerfile.st',
                   help='Specify Squeakerfile to use in <directory>')
    p.add_argument('-t', metavar='tag', type=str, default=None,
                   help='Optionally tag the produced image with this name')
    p.add_argument('--no-cache-urls', dest='no_cache', action='append_const', const='url',
                   help='Always redownload from URLs')
    p.add_argument('--no-cache-stages', dest='no_cache', action='append_const', const='stage',
                   help='Always recompute build stages')
    p.add_argument('--no-headless', dest='headless', action='store_false', default=True,
                   help='Run squeak in graphical mode, showing the window')
    p.add_argument('--vm', type=str, default='squeak',
                   help='Specify VM executable name')
    p.add_argument('directory', type=str,
                   help='Directory to build the image in')
    p.set_defaults(no_cache=[], handler=command_build)

    p = sp.add_parser('run', help='Run image')
    p.add_argument('--vm', type=str, default='squeak',
                   help='Specify VM executable name')
    p.add_argument('--headless', dest='headless', action='store_true', default=False,
                   help='Run squeak with a dummy display, not showing the window')
    p.add_argument('image')
    p.add_argument('args', nargs=argparse.REMAINDER)
    p.set_defaults(handler=command_run)

    p = sp.add_parser('gc', help='Garbage-collect images, stages etc.')
    p.add_argument('-n', '--dry-run', dest='dry_run', action='store_true', default=False,
                   help='Show what would be garbage-collected without deleting anything')
    p.add_argument('--delete-unreferenced-urls', dest='delete_urls', action='store_true', default=False,
                   help='Delete unreferenced downloaded image files')
    p.add_argument('--delete-all-urls', dest='delete_urls', action='store_const', const='all',
                   help='Delete all downloaded image files')
    p.add_argument('--keep-all-intermediate', dest='keep_intermediate_stages', action='store_const', const=math.inf, default=1,
                   help='Keep all intermediate stage images for each tag')
    p.add_argument('--keep-intermediate', metavar='N', dest='keep_intermediate_stages', type=int,
                   help='Keep most-recent N intermediate stage images for each tag')
    p.set_defaults(handler=command_gc)

    p = sp.add_parser('tags', help='List available tagged images')
    p.set_defaults(handler=command_tags)

    p = sp.add_parser('resolve-tag', help='Resolve a tagged image to an on-disk path')
    p.add_argument('tag')
    p.set_defaults(handler=command_resolve_tag)

    p = sp.add_parser('dot', help='Produce graphviz dot description of objects')
    p.set_defaults(handler=command_dot)

    args = parser.parse_args(argv)
    logging.basicConfig(level=logging.DEBUG if args.debug else logging.INFO,
                        handlers=[CustomHandler()])
    try:
        args.handler(args)
    except Exception as e:
        logging.error(str(e), exc_info=e if args.debug else False)
        sys.exit(1)

if __name__=='__main__':
    main(sys.argv)
